---
title: "My Coursera Machine Learning Project"
author: "Joe Krebs"
date: "11/10/2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(caret)
require(gbm)
require(randomForest)
require(plyr)
```

## 1. Overview  
The purpose of this project is to use quantified movement data collected from accelerometers on the belt, forearm, arm, and dumbell of 6 study participants to determine how well they perform exercise activities. Outcomes are tracked for correct and 5 different incorrect outcome classifications. We will use machine learning techniques to attempt to classify outcome class from collected feature data.

## 2. Data Loading  
Here we load the training and testing data sets. "#DIV/0!" entries are converted to NA, "user_name", "new_window", "classe", and "problem_id" columns are created as factors and remaining columns are created as numeric or integer.
```{r loading, cache=TRUE}
pml.training <- read.csv(file="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
                         na.strings = "#DIV/0!",
                         colClasses = c("user_name"="factor", "new_window"="factor", "classe"="factor"),
                         as.is = TRUE)
pml.testing <- read.csv(file="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
                        na.strings = "#DIV/0!",
                        colClasses = c("user_name"="factor", "new_window"="factor", "problem_id"="factor"),
                        as.is = TRUE)
```

## 3. Training Data Partitioning  
We partition the input training data set into training (70%) and validation (30%) sets.  
```{r partitioning}
set.seed(123)    
inTrain <- createDataPartition(y=pml.training$classe,
                               p=0.7,
                               list=FALSE)
training <- pml.training[inTrain,]
validation <- pml.training[-inTrain,]
```

## 4. Data Cleansing and Preparation  
To be useful as predictors, feature variables (columns) should have sufficient variance and be reasonably dense and uncorrelated with each other. The training data contains many unuseful columns, which should be removed. 

### Column Removal
The data is cleansed by removing the following:

* columns containg serial observation number and observation times
* columns with near zero variance
* columns containing > 50% NA
* columns having > 75% correlation with other columns (keep one, drop others)
```{r cleansing1, cache=TRUE}
# Drop observation number and time
training[, c(1, 3:5)] <- NULL

# Drop columns with insufficient variance
training[, nearZeroVar(training, saveMetrics=FALSE)] <- NULL

# Drop columns with > 50% NA values
cutoff <- 0.5
drop.cols <- NULL
for (clm in 1:ncol(training)) {
    if (length(which(is.na(training[,clm]))) > cutoff * nrow(training)) {
        drop.cols <- union(drop.cols, clm)
    }
}
training[, drop.cols] <- NULL

# Drop columns with > 75% correlation with other columns
cutoff <- 0.75
drop.cols <- NULL
cr <- cor(training[, c(2:54)])
cr[lower.tri(cr, diag=TRUE)] <- NA
for (rw in 1:(nrow(cr) - 1)) {
    for (cl in (rw + 1):ncol(cr)) {
        if (abs(cr[rw, cl]) > cutoff) drop.cols <- union(drop.cols, cl + 1)
    }
}
training[, drop.cols] <- NULL
```

### Post-Cleansing
The post-cleansing data frame contains `r ncol(training)` columns, including the dependent variable "classe" and `r ncol(training) - 1` independent variables. Detail below.  
```{r cleansing2}
str(training)
```

## 5. Candidate Model Fits
Since the problem is one of classification (the dependent variable is a factor), we explore model fits for the following candidate models:

* regression tree ("rpart")
* gradient boosting ("gbm")
* random forest ("rf")  

### Features  
For each, we include all features and preprocess numeric columns by centering and scaling.  

### Cross Validation  
K-fold cross validation with k=5 folds is used to prevent overfitting.  
```{r explore_echo, eval=FALSE}
set.seed(234)
tr <- trainControl(method = "cv", number = 5)
rpart.fit <- train(classe~., data=training, preProcess=c("center","scale"), 
                   method="rpart", trControl=tr)
gbm.fit <- train(classe~., data=training, preProcess=c("center","scale"), 
                 method="gbm", trControl=tr)
rf.fit <- train(classe~., data=training, preProcess=c("center","scale"), 
                method="rf", trControl=tr)
```

```{r explore, include=FALSE, cache=TRUE}
set.seed(234)
tr <- trainControl(method = "cv", number = 5)
rpart.fit <- train(classe~., data=training, preProcess=c("center","scale"), 
                   method="rpart", trControl=tr)
gbm.fit <- train(classe~., data=training, preProcess=c("center","scale"), 
                 method="gbm", trControl=tr)
rf.fit <- train(classe~., data=training, preProcess=c("center","scale"), 
                method="rf", trControl=tr)
```

### Model Fits  
Both gradient boosting and random forest methods produce very high within-training accuracy, but regression tree is significantly less accurate.  
```{r explore2}
print(rpart.fit)
print(gbm.fit)
print(rf.fit)
```

### Out-of-Sample Accuracy
For gradient boosting and random forest, we test out-of-sample accuracy using the validation hold-out sample. Both methods are highly accurate on the validation sample, but random forest is slightly better, with accuracy of 99.97% (error rate of just 0.03%).

#### Gradient Boosting Confusion Matrix:
```{r explore3}
gbm.predict <- predict(gbm.fit, newdata=validation)
print(confusionMatrix(data=gbm.predict, reference=validation$classe))
```

#### Random Forest Confusion Matrix:
```{r explore4}
rf.predict <- predict(rf.fit, newdata=validation)
print(confusionMatrix(data=rf.predict, reference=validation$classe))
```

## 6. Results

### Selected Model - Random Forest  
The most accurate model proves to be the random forest, with a near perfect accuracy on the validation sample.  

### Random Forest Model Variable Importance
```{r res1}
plot(varImp(rf.fit))
```

### Random Forest Final Model Summary
```{r res2}
print(summary(rf.fit$finalModel))
```

### Prediction on Test Cases  
Using the final random forest model, prediction on the "pml-testing" data is as follows:
```{r res3}
rf.predict.test <- predict(rf.fit, newdata=pml.testing)
print(rf.predict.test)
```
